# RLM-OpenCode

> **True RLM for AI Coding Assistants**

A Recursive Language Model implementation that gives AI coding assistants unlimited context through tool-based access.

## What is this?

RLM-OpenCode is based on the [Recursive Language Models paper (arXiv:2512.24601)](https://arxiv.org/abs/2512.24601). It enables AI coding assistants to handle **242M+ character contexts** by treating context as an external resource accessed via tools, rather than stuffing everything into the prompt.

```
Traditional:  Context (60M chars) â†’ Model â†’ ğŸ’€ FAILS
RLM-OpenCode: Context metadata â†’ Model â†’ Tools â†’ Context chunks â†’ SUCCESS
```

## Quick Start

### Install

```bash
pip install -e .
```

### Add to OpenCode

```bash
rlm-opencode-setup install
```

### Use

```bash
opencode -m rlm-opencode/rlm-internal.rlm-core-v1
```

## How It Works

### The Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Codebase: 60M chars               â”‚
â”‚  Model Context Window: 200K tokens      â”‚
â”‚                                         â”‚
â”‚  60M / 800K = 73x too much!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The RLM Solution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Model receives METADATA only:       â”‚
â”‚     "Context: 60M chars, 5000 lines"    â”‚
â”‚                                         â”‚
â”‚  2. Model gets TOOLS:                   â”‚
â”‚     - rlm_get_context(offset, length)   â”‚
â”‚     - rlm_search(pattern)               â”‚
â”‚     - rlm_find(text)                    â”‚
â”‚                                         â”‚
â”‚  3. Model calls tools on-demand         â”‚
â”‚     to peek/search context              â”‚
â”‚                                         â”‚
â”‚  Result: Unlimited context access!      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Context Tools

The model has access to these tools:

| Tool | Description |
|------|-------------|
| `rlm_get_context(offset, length)` | Get a chunk of context |
| `rlm_search(pattern, max_results)` | Search with regex |
| `rlm_find(text, max_results)` | Find exact text |
| `rlm_stats()` | Get context statistics |
| `rlm_get_entries(type)` | List context entries |

## Architecture

```
OpenCode (Client)                    RLM-OpenCode Server
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   API Request    â”‚                      â”‚
â”‚  opencode run   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI Server      â”‚
â”‚  -m rlm-opencodeâ”‚                  â”‚  (port 8769)         â”‚
â”‚                 â”‚                  â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   Tool Calls     â”‚                      â”‚
â”‚  Model (LLM)    â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Context Store        â”‚
â”‚                 â”‚   Context Data   â”‚  (Session Files)     â”‚
â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  242M+ chars         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Comparison

| Feature | rlm-server | rlm-opencode |
|---------|------------|--------------|
| Context Access | Code execution | Tool calls |
| Integration | Single-shot API | Full OpenCode |
| Natural Feel | Script-like | Conversational |
| Permissions | Sandbox | Handled by OpenCode |
| Best For | Batch processing | Agentic workflows |

## Files

```
rlm-opencode/
â”œâ”€â”€ src/rlm_opencode/
â”‚   â”œâ”€â”€ server.py          # True RLM server
â”‚   â”œâ”€â”€ context_tools.py   # Tool definitions
â”‚   â”œâ”€â”€ session.py         # Context storage
â”‚   â”œâ”€â”€ providers/         # Model API clients
â”‚   â””â”€â”€ cli.py             # CLI commands
â”œâ”€â”€ ARCHITECTURE.md        # Detailed architecture
â”œâ”€â”€ README.md              # This file
â””â”€â”€ pyproject.toml
```

## Setup Details

`rlm-opencode-setup install` modifies your `~/.config/opencode/opencode.json`:

- Adds an **`rlm-opencode`** provider pointing to `http://localhost:8769/v1`
- Creates model entries for **every** model in your existing OpenCode config under the `rlm-opencode/` prefix
- Sets context limit to **67M tokens** (~300M chars) â€” tells OpenCode to send the full conversation history
- The server truncates this to fit the upstream model's real context window (default 128K tokens)

After setup, all your models are available as `rlm-opencode/<provider>.<model>`:

```bash
opencode -m rlm-opencode/your_provider.your_model
```

## Configuration

All thresholds are configurable via environment variables:

| Env Var | Default | Description |
|---------|---------|-------------|
| `RLM_UPSTREAM_MAX_TOKENS` | 128,000 | Upstream model's real context window |
| `RLM_TOKEN_RESERVE` | 16,000 | Reserved tokens for response + tools |
| `RLM_CAPTURE_MIN_CHARS` | 500 | Min chars to capture tool results |
| `RLM_USER_MIN_CHARS` | 0 | Min chars to capture user messages (0 = all) |
| `RLM_ASSISTANT_MIN_CHARS` | 50 | Min chars to capture assistant responses |
| `RLM_CAPTURE_MAX_CHARS` | 50,000 | Max chars per context entry |

## CLI Commands

```bash
rlm-opencode serve          # Start server (foreground)
rlm-opencode serve --bg     # Start in background
rlm-opencode stop           # Stop the server
rlm-opencode restart        # Stop â†’ reinstall â†’ restart
rlm-opencode sessions       # List sessions with context stats
rlm-opencode status         # Server status + env config
rlm-opencode log -f         # Follow server log
rlm-opencode clear --all    # Clear all sessions and logs
```

## Session Data

Sessions are stored in `~/.local/share/rlm-opencode/`:

- `sessions.db` â€” Session metadata (SQLite)
- `contexts/` â€” Context files (one per session, append-only)

Context accumulates across OpenCode calls, persisting between sessions. Each OpenCode chat gets its own isolated session via request fingerprinting.

## License

MIT
