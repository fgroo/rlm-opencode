# RLM-OpenCode

> **True RLM for AI Coding Assistants**

A Recursive Language Model implementation that gives AI coding assistants unlimited context through tool-based access.

## What is this?

RLM-OpenCode is based on the [Recursive Language Models paper (arXiv:2512.24601)](https://arxiv.org/abs/2512.24601). It enables AI coding assistants to handle **100M+ character contexts** by treating context as an external resource accessed via tools, rather than stuffing everything into the prompt.

```
Traditional:  Context (60M chars) â†’ Model â†’ ğŸ’€ FAILS
RLM-OpenCode: Context metadata â†’ Model â†’ Tools â†’ Context chunks â†’ SUCCESS
```

## Quick Start

### Install

```bash
pip install -e .
```

### Add to OpenCode

```bash
rlm-opencode-setup install
```

### Use

```bash
opencode -m rlm-opencode/rlm-internal.rlm-core-v1
```

## How It Works

### The Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Codebase: 60M chars               â”‚
â”‚  Model Context Window: 200K tokens      â”‚
â”‚                                         â”‚
â”‚  60M / 800K = 73x too much!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The RLM Solution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Model receives METADATA only:       â”‚
â”‚     "Context: 60M chars, 5000 lines"    â”‚
â”‚                                         â”‚
â”‚  2. Model gets TOOLS:                   â”‚
â”‚     - rlm_get_context(offset, length)   â”‚
â”‚     - rlm_search(pattern)               â”‚
â”‚     - rlm_find(text)                    â”‚
â”‚                                         â”‚
â”‚  3. Model calls tools on-demand         â”‚
â”‚     to peek/search context              â”‚
â”‚                                         â”‚
â”‚  Result: Unlimited context access!      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Context Tools

The model has access to these tools:

| Tool | Description |
|------|-------------|
| `rlm_get_context(offset, length)` | Get a chunk of context |
| `rlm_search(pattern, max_results)` | Search with regex |
| `rlm_find(text, max_results)` | Find exact text |
| `rlm_stats()` | Get context statistics |
| `rlm_get_entries(type)` | List context entries |

## Architecture

```
OpenCode (Client)                    RLM-OpenCode Server
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   API Request    â”‚                      â”‚
â”‚  opencode run   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  FastAPI Server      â”‚
â”‚  -m rlm-opencodeâ”‚                  â”‚  (port 8769)         â”‚
â”‚                 â”‚                  â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   Tool Calls     â”‚                      â”‚
â”‚  Model (GLM-5)  â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  Context Store       â”‚
â”‚                 â”‚   Context Data   â”‚  (Session Files)     â”‚
â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  100M+ chars         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Comparison

| Feature | rlm-server | rlm-opencode |
|---------|------------|--------------|
| Context Access | Code execution | Tool calls |
| Integration | Single-shot API | Full OpenCode |
| Natural Feel | Script-like | Conversational |
| Permissions | Sandbox | Handled by OpenCode |
| Best For | Batch processing | Agentic workflows |

## Files

```
rlm-opencode/
â”œâ”€â”€ src/rlm_opencode/
â”‚   â”œâ”€â”€ server.py          # True RLM server
â”‚   â”œâ”€â”€ context_tools.py   # Tool definitions
â”‚   â”œâ”€â”€ session.py         # Context storage
â”‚   â”œâ”€â”€ providers/         # Model API clients
â”‚   â””â”€â”€ cli.py             # CLI commands
â”œâ”€â”€ ARCHITECTURE.md        # Detailed architecture
â”œâ”€â”€ README.md              # This file
â””â”€â”€ pyproject.toml
```

## Session Data

Sessions are stored in `~/.local/share/rlm-opencode/`:

- `sessions/` - Context files and metadata
- `mappings/` - Directory â†’ Session mapping

Context accumulates across OpenCode calls, persisting between sessions.

## License

MIT
