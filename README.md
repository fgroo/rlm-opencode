# RLM-OpenCode

> **True RLM for AI Coding Assistants**

A Recursive Language Model implementation that gives AI coding assistants unlimited context through tool-based access.

## What is this?

RLM-OpenCode is based on the [Recursive Language Models paper (arXiv:2512.24601)](https://arxiv.org/abs/2512.24601). It enables AI coding assistants to handle **242M+ character contexts** by treating context as an external resource accessed via tools, rather than stuffing everything into the prompt.

```
Traditional:  Context (60M chars) â†’ Model â†’ ğŸ’€ FAILS
RLM-OpenCode: Context metadata â†’ Model â†’ Tools â†’ Context chunks â†’ SUCCESS
```

## Quick Start

### Install

```bash
pip install -e .
```

### Add to OpenCode

```bash
rlm-opencode-setup install
```

### Use

```bash
opencode -m rlm-opencode/rlm-internal.rlm-core-v1
```

## How It Works

### The Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Your Codebase: 60M chars               â”‚
â”‚  Model Context Window: 200K tokens      â”‚
â”‚                                         â”‚
â”‚  60M / 800K = 73x too much!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The RLM Solution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Model receives METADATA only:       â”‚
â”‚     "Context: 60M chars, 5000 lines"    â”‚
â”‚                                         â”‚
â”‚  2. Model gets TOOLS:                   â”‚
â”‚     - rlm_get_context(offset, length)   â”‚
â”‚     - rlm_search(pattern)               â”‚
â”‚     - rlm_find(text)                    â”‚
â”‚                                         â”‚
â”‚  3. Model calls tools on-demand         â”‚
â”‚     to peek/search context              â”‚
â”‚                                         â”‚
â”‚  Result: Unlimited context access!      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Context Tools

The model has access to these tools:

| Tool | Description |
|------|-------------|
| `rlm_get_context(offset, length)` | Get a chunk of context |
| `rlm_search(pattern, max_results)` | Search with regex |
| `rlm_find(text, max_results)` | Find exact text |
| `rlm_stats()` | Get context statistics |
| `rlm_get_entries(type)` | List context entries |

## Architecture

```
OpenCode (Client)                    RLM-OpenCode Server
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   API Request    â”‚                      â”‚
â”‚  opencode run   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI Server      â”‚
â”‚  -m rlm-opencodeâ”‚                  â”‚  (port 8769)         â”‚
â”‚                 â”‚                  â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   Tool Calls     â”‚                      â”‚
â”‚  Model (LLM)    â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Context Store        â”‚
â”‚                 â”‚   Context Data   â”‚  (Session Files)     â”‚
â”‚                 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  242M+ chars         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Comparison

| Feature | rlm-server | rlm-opencode |
|---------|------------|--------------|
| Context Access | Code execution | Tool calls |
| Integration | Single-shot API | Full OpenCode |
| Natural Feel | Script-like | Conversational |
| Permissions | Sandbox | Handled by OpenCode |
| Best For | Batch processing | Agentic workflows |

## Files

```
rlm-opencode/
â”œâ”€â”€ src/rlm_opencode/
â”‚   â”œâ”€â”€ server.py          # True RLM server
â”‚   â”œâ”€â”€ context_tools.py   # Tool definitions
â”‚   â”œâ”€â”€ session.py         # Context storage
â”‚   â”œâ”€â”€ providers/         # Model API clients
â”‚   â””â”€â”€ cli.py             # CLI commands
â”œâ”€â”€ ARCHITECTURE.md        # Detailed architecture
â”œâ”€â”€ README.md              # This file
â””â”€â”€ pyproject.toml
```

## Setup Details

`rlm-opencode-setup install` modifies your `~/.config/opencode/opencode.json`:

- Adds an **`rlm-opencode`** provider pointing to `http://localhost:8769/v1`
- Creates model entries for **every** model in your existing OpenCode config under the `rlm-opencode/` prefix
- Sets context limit to **67M tokens** (~300M chars) â€” tells OpenCode to send the full conversation history
- The server truncates this to fit the upstream model's real context window (default 128K tokens)

After setup, all your models are available as `rlm-opencode/<provider>.<model>`:

```bash
opencode -m rlm-opencode/your_provider.your_model
```

## Configuration

All thresholds are dynamically configurable via the built-in CLI config tool (which saves to a persistent JSON store) or via environment variables (which act as fallbacks).

```bash
rlm-opencode config rlm_max_payload_chars 75000
```

| Config Key / Env Var | Default | Description |
|----------------------|---------|-------------|
| `strict_mode_level` | 0 | [0-4] Forces LLM to rely on tools (4 = Maximum Amnesia) |
| `rlm_upstream_max_tokens` | 128,000 | The raw token limit of the underlying LLM's architecture |
| `rlm_token_reserve` | 16,000 | Budget reserved for the model's generation output and tool responses |
| `rlm_max_payload_chars` | 250,000 | Absolute size limit of the immediate workspace injected natively |
| `rlm_capture_min_chars` | 500 | Min chars for a tool result to be saved into context (0 = all) |
| `rlm_capture_max_chars` | 50,000 | Max chars per single entry in the context lake before truncation |
| `rlm_user_min_chars` | 0 | Min chars for a user message to be captured (0 = all) |
| `rlm_assistant_min_chars` | 50 | Min chars for an assistant response to be captured |
| `rlm_summarize_model` | None | Specific model override for rlm_summarize tool calls |

## CLI Commands

```bash
rlm-opencode serve          # Start server (foreground)
rlm-opencode serve --bg     # Start in background
rlm-opencode stop           # Stop the server
rlm-opencode restart        # Stop â†’ reinstall â†’ restart
rlm-opencode config         # View or set persistent settings
rlm-opencode branch         # Branch, backup, restore, or transfer session memory
rlm-opencode log [-f]       # View visually rendered session history tree (or follow logs)
rlm-opencode sessions       # List sessions with context stats
rlm-opencode status         # Server status + active configuration
rlm-opencode clear --all    # Clear all sessions and logs
```

## Session Data

Sessions are stored in `~/.local/share/rlm-opencode/`:

- `sessions.db` â€” Session metadata (SQLite)
- `contexts/` â€” Context files (one per session, append-only)

Context accumulates across OpenCode calls, persisting between sessions. Each OpenCode chat gets its own isolated session via request fingerprinting.

## License

MIT
